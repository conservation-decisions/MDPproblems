##################################################################################
exact_policy <- rep(0,K)
for (k in 0:K) {
exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}
# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy
##################################################################################
# MARESCOT ET AL.
# COMPLEX DECISIONS MADE SIMPLE: A PRIMER ON STOCHASTIC DYNAMIC PROGRAMMING
##################################################################################
##################################################################################
# APPENDIX 5: STOCHASTIC DYNAMIC PROGRAMMING MODEL WITH DEMOGRAPHIC STOCHASTICITY
# Computation time ~ 1 min. on an Intel Xeon CPU X5670 Westmere @2.93 GHz
##################################################################################
##################################################################################
# STEP 1: DEFINE OBJECTIVES
##################################################################################
# This is a conceptual step which does not require coding
###################################################################################
# STEP 2: DEFINE STATES
##################################################################################
# state space limit
K <- 199
# Vector of all possible states
states <- 0:K
##################################################################################
# STEP 3: DEFINE CONTROL ACTIONS
##################################################################################
# Vector of actions: rate of the population that can be removed, ranging #from 0 to 1
H <- seq(0, 1, 1/(K+1))
#H <- seq(0, 1, 0.1)
##################################################################################
# STEP 4: DEFINE DYNAMIC MODEL (WITH DEMOGRAPHIC PARAMETERS)
##################################################################################
# Population growth rate
lambda <- 1.25
# Function for the exponential growth of the dynamic model
dynamic <- function(actualpop, action) {
nextpop <- actualpop*lambda*(1-action)
return(nextpop)
}
##################################################################################
# STEP 5: DEFINE UTILITY
##################################################################################
# Maximum objective threshold for population abundance
Nmax <- 250
# Minimum objective threshold for population abundance
Nmin <- 50
# Utility function
get_utility <- function(x) {
return(ifelse(x < Nmin | x > Nmax, 0, x))
}
##################################################################################
# STEP 6: SOLVE BELLMAN EQUATION WITH VALUE ITERATION
##################################################################################
# Transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
# Utility matrix
utility <- array(0, dim = c(length(states), length(H)))
# Fill in the transition and utility matrix
# Loop on all states
for (k in 0:K) {
# Loop on all actions
for (i in 1:length(H)) {
# Calculate the transition state at the next step, given the #current state k and the harvest Hi
nextpop <- dynamic(k, H[i])
# Implement demographic stochasticity by drawing
#probability from a Poisson density function
transition[k+1,,i] <- dpois(states,nextpop)
# We need to correct this density for the final capping state
transition[k+1,K+1,i] <- 1 - sum(transition[k+1,-(K+1),i])
# Compute utility
utility[k+1,i] <- get_utility(nextpop)
} # end of action loop
} # end of state loop
for (f in 1:length(H)) {
new_P_file = paste("P",f,length(states),length(H),".txt",sep="_");
write.table(transition[,,f], file=new_P_file, row.names=FALSE, col.names=FALSE)
}
R_file = paste("R",length(states),length(H),".txt",sep="_");
write.table(utility, file=R_file, row.names=FALSE, col.names=FALSE)
transition
utility
# Discount factor
discount <- 0.9
# Action value vector at tmax
Vtmax <- numeric(length(states))
# Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
# Optimal policy vector
D <- numeric(length(states))
# Time horizon
Tmax <- 150
# The backward iteration consists in storing action values in the vector Vt which is the maximum of
# utility plus the future action values for all possible next states. Knowing the final action
# values, we can then backwardly reset the next action value Vtplus to the new value Vt. We start
# The backward iteration at time T-1 since we already defined the action #value at Tmax.
for (t in (Tmax-1):1) {
# We define a matrix Q that stores the updated action values for #all states (rows)
# actions (columns)
Q <- array(0, dim=c(length(states), length(H)))
for (i in 1:length(H)) {
# For each harvest rate we fill for all states values (row) #the ith column (Action) of matrix Q
# The utility of the ith action recorded for all states is #added to the product of the transition matrix of the ith #action by the action value of all states
Q[,i] <- utility[,i] + discount*(transition[,,i] %*% Vtplus)
} # end of the harvest loop
# Find the optimal action value at time t is the maximum of Q
Vt <- apply(Q, 1, max)
# After filling vector Vt of the action values at all states, we #update the vector Vt+1 to Vt and we go to the next step standing #for previous time t-1, since we iterate backward
Vtplus <- Vt
} # end of the time loop
# Find optimal action for each state
for (k in 0:K) {
# We look for each state which column of Q corresponds to the #maximum of the last updated value
# of Vt (the one at time t+1). If the index vector is longer than 1 #(if there is more than one optimal value we chose the minimum #harvest rate)
D[k+1] <- H[(min(which(Q[k+1,] == Vt[k+1])))]
}
##################################################################################
# PLOT SOLUTION
##################################################################################
plot(states, D, xlab="Population size", ylab="harvest rate")
##################################################################################
# PROOF OF OPTIMALITY: COMPARE WITH ANALYTICAL SOLUTION
##################################################################################
exact_policy <- rep(0,K)
for (k in 0:K) {
exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}
# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy
##################################################################################
# MARESCOT ET AL.
# COMPLEX DECISIONS MADE SIMPLE: A PRIMER ON STOCHASTIC DYNAMIC PROGRAMMING
##################################################################################
##################################################################################
# APPENDIX 5: STOCHASTIC DYNAMIC PROGRAMMING MODEL WITH DEMOGRAPHIC STOCHASTICITY
# Computation time ~ 1 min. on an Intel Xeon CPU X5670 Westmere @2.93 GHz
##################################################################################
##################################################################################
# STEP 1: DEFINE OBJECTIVES
##################################################################################
# This is a conceptual step which does not require coding
###################################################################################
# STEP 2: DEFINE STATES
##################################################################################
# state space limit
K <- 249
# Vector of all possible states
states <- 0:K
##################################################################################
# STEP 3: DEFINE CONTROL ACTIONS
##################################################################################
# Vector of actions: rate of the population that can be removed, ranging #from 0 to 1
H <- seq(0, 1, 1/(K+1))
#H <- seq(0, 1, 0.1)
##################################################################################
# STEP 4: DEFINE DYNAMIC MODEL (WITH DEMOGRAPHIC PARAMETERS)
##################################################################################
# Population growth rate
lambda <- 1.25
# Function for the exponential growth of the dynamic model
dynamic <- function(actualpop, action) {
nextpop <- actualpop*lambda*(1-action)
return(nextpop)
}
##################################################################################
# STEP 5: DEFINE UTILITY
##################################################################################
# Maximum objective threshold for population abundance
Nmax <- 250
# Minimum objective threshold for population abundance
Nmin <- 50
# Utility function
get_utility <- function(x) {
return(ifelse(x < Nmin | x > Nmax, 0, x))
}
##################################################################################
# STEP 6: SOLVE BELLMAN EQUATION WITH VALUE ITERATION
##################################################################################
# Transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
# Utility matrix
utility <- array(0, dim = c(length(states), length(H)))
# Fill in the transition and utility matrix
# Loop on all states
for (k in 0:K) {
# Loop on all actions
for (i in 1:length(H)) {
# Calculate the transition state at the next step, given the #current state k and the harvest Hi
nextpop <- dynamic(k, H[i])
# Implement demographic stochasticity by drawing
#probability from a Poisson density function
transition[k+1,,i] <- dpois(states,nextpop)
# We need to correct this density for the final capping state
transition[k+1,K+1,i] <- 1 - sum(transition[k+1,-(K+1),i])
# Compute utility
utility[k+1,i] <- get_utility(nextpop)
} # end of action loop
} # end of state loop
for (f in 1:length(H)) {
new_P_file = paste("P",f,length(states),length(H),".txt",sep="_");
write.table(transition[,,f], file=new_P_file, row.names=FALSE, col.names=FALSE)
}
R_file = paste("R",length(states),length(H),".txt",sep="_");
write.table(utility, file=R_file, row.names=FALSE, col.names=FALSE)
transition
utility
# Discount factor
discount <- 0.9
# Action value vector at tmax
Vtmax <- numeric(length(states))
# Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
# Optimal policy vector
D <- numeric(length(states))
# Time horizon
Tmax <- 150
# The backward iteration consists in storing action values in the vector Vt which is the maximum of
# utility plus the future action values for all possible next states. Knowing the final action
# values, we can then backwardly reset the next action value Vtplus to the new value Vt. We start
# The backward iteration at time T-1 since we already defined the action #value at Tmax.
for (t in (Tmax-1):1) {
# We define a matrix Q that stores the updated action values for #all states (rows)
# actions (columns)
Q <- array(0, dim=c(length(states), length(H)))
for (i in 1:length(H)) {
# For each harvest rate we fill for all states values (row) #the ith column (Action) of matrix Q
# The utility of the ith action recorded for all states is #added to the product of the transition matrix of the ith #action by the action value of all states
Q[,i] <- utility[,i] + discount*(transition[,,i] %*% Vtplus)
} # end of the harvest loop
# Find the optimal action value at time t is the maximum of Q
Vt <- apply(Q, 1, max)
# After filling vector Vt of the action values at all states, we #update the vector Vt+1 to Vt and we go to the next step standing #for previous time t-1, since we iterate backward
Vtplus <- Vt
} # end of the time loop
# Find optimal action for each state
for (k in 0:K) {
# We look for each state which column of Q corresponds to the #maximum of the last updated value
# of Vt (the one at time t+1). If the index vector is longer than 1 #(if there is more than one optimal value we chose the minimum #harvest rate)
D[k+1] <- H[(min(which(Q[k+1,] == Vt[k+1])))]
}
##################################################################################
# PLOT SOLUTION
##################################################################################
plot(states, D, xlab="Population size", ylab="harvest rate")
##################################################################################
# PROOF OF OPTIMALITY: COMPARE WITH ANALYTICAL SOLUTION
##################################################################################
exact_policy <- rep(0,K)
for (k in 0:K) {
exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}
# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy
##################################################################################
# MARESCOT ET AL.
# COMPLEX DECISIONS MADE SIMPLE: A PRIMER ON STOCHASTIC DYNAMIC PROGRAMMING
##################################################################################
##################################################################################
# APPENDIX 5: STOCHASTIC DYNAMIC PROGRAMMING MODEL WITH DEMOGRAPHIC STOCHASTICITY
# Computation time ~ 1 min. on an Intel Xeon CPU X5670 Westmere @2.93 GHz
##################################################################################
##################################################################################
# STEP 1: DEFINE OBJECTIVES
##################################################################################
# This is a conceptual step which does not require coding
###################################################################################
# STEP 2: DEFINE STATES
##################################################################################
# state space limit
K <- 1999
# Vector of all possible states
states <- 0:K
##################################################################################
# STEP 3: DEFINE CONTROL ACTIONS
##################################################################################
# Vector of actions: rate of the population that can be removed, ranging #from 0 to 1
#H <- seq(0, 1, 1/(K+1))
H <- seq(0, 1, 0.1)
##################################################################################
# STEP 4: DEFINE DYNAMIC MODEL (WITH DEMOGRAPHIC PARAMETERS)
##################################################################################
# Population growth rate
lambda <- 1.25
# Function for the exponential growth of the dynamic model
dynamic <- function(actualpop, action) {
nextpop <- actualpop*lambda*(1-action)
return(nextpop)
}
##################################################################################
# STEP 5: DEFINE UTILITY
##################################################################################
# Maximum objective threshold for population abundance
Nmax <- 2000
# Minimum objective threshold for population abundance
Nmin <- 400
# Utility function
get_utility <- function(x) {
return(ifelse(x < Nmin | x > Nmax, 0, x))
}
##################################################################################
# STEP 6: SOLVE BELLMAN EQUATION WITH VALUE ITERATION
##################################################################################
# Transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
# Utility matrix
utility <- array(0, dim = c(length(states), length(H)))
# Fill in the transition and utility matrix
# Loop on all states
for (k in 0:K) {
# Loop on all actions
for (i in 1:length(H)) {
# Calculate the transition state at the next step, given the #current state k and the harvest Hi
nextpop <- dynamic(k, H[i])
# Implement demographic stochasticity by drawing
#probability from a Poisson density function
transition[k+1,,i] <- dpois(states,nextpop)
# We need to correct this density for the final capping state
transition[k+1,K+1,i] <- 1 - sum(transition[k+1,-(K+1),i])
# Compute utility
utility[k+1,i] <- get_utility(nextpop)
} # end of action loop
} # end of state loop
for (f in 1:length(H)) {
new_P_file = paste("P",f,length(states),length(H),".txt",sep="_");
write.table(transition[,,f], file=new_P_file, row.names=FALSE, col.names=FALSE)
}
R_file = paste("R",length(states),length(H),".txt",sep="_");
write.table(utility, file=R_file, row.names=FALSE, col.names=FALSE)
transition
utility
# Discount factor
discount <- 0.9
# Action value vector at tmax
Vtmax <- numeric(length(states))
# Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
# Optimal policy vector
D <- numeric(length(states))
# Time horizon
Tmax <- 150
# The backward iteration consists in storing action values in the vector Vt which is the maximum of
# utility plus the future action values for all possible next states. Knowing the final action
# values, we can then backwardly reset the next action value Vtplus to the new value Vt. We start
# The backward iteration at time T-1 since we already defined the action #value at Tmax.
for (t in (Tmax-1):1) {
# We define a matrix Q that stores the updated action values for #all states (rows)
# actions (columns)
Q <- array(0, dim=c(length(states), length(H)))
for (i in 1:length(H)) {
# For each harvest rate we fill for all states values (row) #the ith column (Action) of matrix Q
# The utility of the ith action recorded for all states is #added to the product of the transition matrix of the ith #action by the action value of all states
Q[,i] <- utility[,i] + discount*(transition[,,i] %*% Vtplus)
} # end of the harvest loop
# Find the optimal action value at time t is the maximum of Q
Vt <- apply(Q, 1, max)
# After filling vector Vt of the action values at all states, we #update the vector Vt+1 to Vt and we go to the next step standing #for previous time t-1, since we iterate backward
Vtplus <- Vt
} # end of the time loop
# Find optimal action for each state
for (k in 0:K) {
# We look for each state which column of Q corresponds to the #maximum of the last updated value
# of Vt (the one at time t+1). If the index vector is longer than 1 #(if there is more than one optimal value we chose the minimum #harvest rate)
D[k+1] <- H[(min(which(Q[k+1,] == Vt[k+1])))]
}
##################################################################################
# PLOT SOLUTION
##################################################################################
plot(states, D, xlab="Population size", ylab="harvest rate")
##################################################################################
# PROOF OF OPTIMALITY: COMPARE WITH ANALYTICAL SOLUTION
##################################################################################
exact_policy <- rep(0,K)
for (k in 0:K) {
exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}
# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy
##################################################################################
# MARESCOT ET AL.
# COMPLEX DECISIONS MADE SIMPLE: A PRIMER ON STOCHASTIC DYNAMIC PROGRAMMING
##################################################################################
##################################################################################
# APPENDIX 5: STOCHASTIC DYNAMIC PROGRAMMING MODEL WITH DEMOGRAPHIC STOCHASTICITY
# Computation time ~ 1 min. on an Intel Xeon CPU X5670 Westmere @2.93 GHz
##################################################################################
##################################################################################
# STEP 1: DEFINE OBJECTIVES
##################################################################################
# This is a conceptual step which does not require coding
###################################################################################
# STEP 2: DEFINE STATES
##################################################################################
# state space limit
K <- 2999
# Vector of all possible states
states <- 0:K
##################################################################################
# STEP 3: DEFINE CONTROL ACTIONS
##################################################################################
# Vector of actions: rate of the population that can be removed, ranging #from 0 to 1
#H <- seq(0, 1, 1/(K+1))
H <- seq(0, 1, 0.1)
##################################################################################
# STEP 4: DEFINE DYNAMIC MODEL (WITH DEMOGRAPHIC PARAMETERS)
##################################################################################
# Population growth rate
lambda <- 1.25
# Function for the exponential growth of the dynamic model
dynamic <- function(actualpop, action) {
nextpop <- actualpop*lambda*(1-action)
return(nextpop)
}
##################################################################################
# STEP 5: DEFINE UTILITY
##################################################################################
# Maximum objective threshold for population abundance
Nmax <- 3000
# Minimum objective threshold for population abundance
Nmin <- 600
# Utility function
get_utility <- function(x) {
return(ifelse(x < Nmin | x > Nmax, 0, x))
}
##################################################################################
# STEP 6: SOLVE BELLMAN EQUATION WITH VALUE ITERATION
##################################################################################
# Transition matrix
transition <- array(0, dim = c(length(states), length(states), length(H)))
# Utility matrix
utility <- array(0, dim = c(length(states), length(H)))
# Fill in the transition and utility matrix
# Loop on all states
for (k in 0:K) {
# Loop on all actions
for (i in 1:length(H)) {
# Calculate the transition state at the next step, given the #current state k and the harvest Hi
nextpop <- dynamic(k, H[i])
# Implement demographic stochasticity by drawing
#probability from a Poisson density function
transition[k+1,,i] <- dpois(states,nextpop)
# We need to correct this density for the final capping state
transition[k+1,K+1,i] <- 1 - sum(transition[k+1,-(K+1),i])
# Compute utility
utility[k+1,i] <- get_utility(nextpop)
} # end of action loop
} # end of state loop
for (f in 1:length(H)) {
new_P_file = paste("P",f,length(states),length(H),".txt",sep="_");
write.table(transition[,,f], file=new_P_file, row.names=FALSE, col.names=FALSE)
}
R_file = paste("R",length(states),length(H),".txt",sep="_");
write.table(utility, file=R_file, row.names=FALSE, col.names=FALSE)
transition
utility
# Discount factor
discount <- 0.9
# Action value vector at tmax
Vtmax <- numeric(length(states))
# Action value vector at t and t+1
Vt <- numeric(length(states))
Vtplus <- numeric(length(states))
# Optimal policy vector
D <- numeric(length(states))
# Time horizon
Tmax <- 150
# The backward iteration consists in storing action values in the vector Vt which is the maximum of
# utility plus the future action values for all possible next states. Knowing the final action
# values, we can then backwardly reset the next action value Vtplus to the new value Vt. We start
# The backward iteration at time T-1 since we already defined the action #value at Tmax.
for (t in (Tmax-1):1) {
# We define a matrix Q that stores the updated action values for #all states (rows)
# actions (columns)
Q <- array(0, dim=c(length(states), length(H)))
for (i in 1:length(H)) {
# For each harvest rate we fill for all states values (row) #the ith column (Action) of matrix Q
# The utility of the ith action recorded for all states is #added to the product of the transition matrix of the ith #action by the action value of all states
Q[,i] <- utility[,i] + discount*(transition[,,i] %*% Vtplus)
} # end of the harvest loop
# Find the optimal action value at time t is the maximum of Q
Vt <- apply(Q, 1, max)
# After filling vector Vt of the action values at all states, we #update the vector Vt+1 to Vt and we go to the next step standing #for previous time t-1, since we iterate backward
Vtplus <- Vt
} # end of the time loop
# Find optimal action for each state
for (k in 0:K) {
# We look for each state which column of Q corresponds to the #maximum of the last updated value
# of Vt (the one at time t+1). If the index vector is longer than 1 #(if there is more than one optimal value we chose the minimum #harvest rate)
D[k+1] <- H[(min(which(Q[k+1,] == Vt[k+1])))]
}
##################################################################################
# PLOT SOLUTION
##################################################################################
plot(states, D, xlab="Population size", ylab="harvest rate")
##################################################################################
# PROOF OF OPTIMALITY: COMPARE WITH ANALYTICAL SOLUTION
##################################################################################
exact_policy <- rep(0,K)
for (k in 0:K) {
exact_policy[k+1] <- max(0, 1 - K/(k*lambda))
}
# The difference between Bellman equation solution and the analytical #solution is small:
lines(states, exact_policy)
D - exact_policy
